defaults:
    # Main environment for loading sub-configurations
  - env: ???
    # Main dynamics model type
  - dynamics_model: pe
    # Environment specific dynamics model specification
  - env/: ${defaults.0.env}/dynamics_model/${defaults.1.dynamics_model}
    # Main optimizer type
  - optimizer: cem
    # Environment specific optimizer specification
  - env/: ${defaults.0.env}/optimizer/${defaults.3.optimizer}
    # Planner specification
  - traj: simple

env:
  name: ???
  # static reward function, reward(state, action) -> float
  # fully qualified function name
  reward_func: ???
  # state transformer function:
  # transformer(state) -> transformed_state
  state_transformer: mbrl.environments.hooks.default_state_transformer
  target_transformer: mbrl.environments.hooks.DefaultTargetTransformer

  render: false
  instances: 1

# load a checkpoint file, this is either false to indicate to not load, or
# a file name of the checkpoint file to load.
checkpoint: false

num_trials: 20
trial_timesteps: 200
device: cuda
random_seed: 1
log_dir: logs
log_config: pets/conf/logging.yaml
checkpoint_file: trial_{}.dat
bc_plus: true

# Definition for each dynamics model type:
dynamics_model:
  clazz: ???

training:
  # Incrementally update the model between trials.
  # if false, model will be trained from scratch for training.full_epochs at every trial
  # if true, model will trained for training.full_epochs in the first trial (from motor babbling),
  # and for subsequent trials it will be updated for training.incremental_epochs
  incremental: false
  full_epochs: 100
  incremental_epochs: 5

  batch_size: 16

  optimizer:
    clazz: torch.optim.Adam
    params:
      lr: 1e-4

  testing:
    # ratio between test and train splits, 0.7 means 70% in train and 30% in test.
    # to use all the data for training and not perform testing set to 1
    split: 0.9


motor_babbling:
  num_trials: 1
  task_horizon: ${trial_timesteps}

policy:
  clazz: pets.PETSPolicy
  params:
    planning_horizon: 25
    traj: ???
    optimizer: ???
